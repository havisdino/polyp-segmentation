{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T16:17:05.624874Z",
     "iopub.status.busy": "2023-06-25T16:17:05.624266Z",
     "iopub.status.idle": "2023-06-25T16:17:05.650114Z",
     "shell.execute_reply": "2023-06-25T16:17:05.648658Z",
     "shell.execute_reply.started": "2023-06-25T16:17:05.624839Z"
    }
   },
   "source": [
    "# POLYP SEGMENTATION\n",
    "\n",
    "## The task\n",
    "Given an endoscopic image, we have to predict a binary mask where 1's pixels are the polyps' regions.\n",
    "\n",
    "## Assumptions\n",
    "Let the mask be a random variable $M$ and the input image is also a random variable $X$. To perform the segmentation task, we estimate the conditional probablity $P(M=m|X=x)$. Due to the assumption that the mask is binary, $P(M=m|X=x)$ is a Bernoulli distribution, which means that $m|x \\sim \\text{Ber}(\\lambda(x))$ where $\\lambda(x)$ is modeled using a neural network $G_\\theta(x)$ ($\\theta$ is a set of the network's parameters). In conclusion, we have to estimate $\\text{Ber}(m|G_\\theta(x))$. Another assumption is that the elements in an arbitrary $m^{<i>}$ are independent for the sake of calculating the later join probabilities.\n",
    "\n",
    "## Methodology\n",
    "Given a set of training data including endoscopic images $(x^{<1>}, x^{<2>}, ..., x^{<N>})$ and target masks $(m^{<1>}, m^{<2>}, ..., m^{<N>})$ (any $x^{<i>}$ or $m^{<i>}$ is vector-valued), our goal is maximizing the conditional log-likelihood of this training dataset:<br>\n",
    "<center>\n",
    "    <div style=\"display: inline-block; text-align: left;\">\n",
    "        $\\theta^* = \\text{argmax}_\\theta\\ \\sum_{i=1}^N \\log P(m^{<i>}|x^{<i>})$<br>\n",
    "        $\\ \\ \\ \\  = \\text{argmax}_\\theta\\ \\sum_{i=1}^N \\log \\text{Ber}(m^{<i>};G_\\theta(x^{<i>}))$<br>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "We have assumed that the elements in an arbitrary $m^{<i>}$ are independent. Therefore:<br>\n",
    "<center>\n",
    "    <div style=\"display: inline-block; text-align: left;\">\n",
    "        $\\theta^* = \\text{argmax}_\\theta\\ \\sum_{i=1}^N \\sum_{k=1}^K \\log \\text{Ber}(m^{<i>}_k;G_\\theta(x^{<i>})_k)$ <br>\n",
    "        $\\ \\ \\ \\ = \\text{argmax}_\\theta\\ \\sum_{i=1}^N \\sum_{k=1}^K \\log G_\\theta(x^{<i>})_k^{m^{<i>}_k} (1 - G_\\theta(x^{<i>})_k)^{1 - m^{<i>}_k}$<br>\n",
    "        $\\ \\ \\ \\ = \\text{argmax}_\\theta\\ \\sum_{i=1}^N \\sum_{k=1}^K {m^{<i>}_k}\\log G_\\theta(x^{<i>})_k + ({1 - m^{<i>}_k})\\log(1 - G_\\theta(x^{<i>})_k)$<br>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "It turns out that we have to optimize a binary cross-entropy loss function.<br>\n",
    "In the code below, the neural network $G_\\theta(x)$ is modeled as a U-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(imgs, n_rows=8):    \n",
    "    imgs = tv.utils.make_grid(imgs, n_rows, padding=4)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(imgs.permute(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, img_size):\n",
    "        super().__init__()\n",
    "        self.img_paths = [\n",
    "            os.path.join(img_dir, path)\n",
    "            for path in os.listdir(img_dir)\n",
    "            if path.endswith('.jpg')\n",
    "            or path.endswith('.jpeg')\n",
    "            or path.endswith('.png')\n",
    "        ]\n",
    "        self.label_paths = [\n",
    "            os.path.join(label_dir, path)\n",
    "            for path in os.listdir(label_dir)\n",
    "            if path.endswith('.jpg')\n",
    "            or path.endswith('.jpeg')\n",
    "            or path.endswith('.png')\n",
    "        ]\n",
    "        assert len(self.img_paths) == len(self.label_paths)\n",
    "        self.resize = transforms.Resize(img_size, antialias=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = tv.io.read_image(self.img_paths[index])\n",
    "        img = self.resize(img) / 255.0\n",
    "        label = tv.io.read_image(self.label_paths[index])\n",
    "        label = self.resize(label)[0] / 255.0\n",
    "        return img, label.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              kernel_size, stride, padding)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels,\n",
    "                                         kernel_size, stride, padding)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.deconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dropout=None):\n",
    "        super().__init__()\n",
    "        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding='same')\n",
    "        self.conv_block_1 = ConvBlock(in_channels, out_channels, kernel_size, padding='same')\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding='same')\n",
    "        self.norm_2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip = self.res_conv(x)\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_2(x) + skip\n",
    "        x = self.norm_2(x)\n",
    "        x = self.relu(x)\n",
    "        if hasattr(self, 'dropout'):\n",
    "            x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dropout=None):\n",
    "        super().__init__()\n",
    "        self.res_block = ResidualBlock(in_channels, out_channels, kernel_size, dropout)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(x)\n",
    "        x = self.res_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dropout=None):\n",
    "        super().__init__()\n",
    "        self.res_block = ResidualBlock(in_channels, out_channels, kernel_size, dropout)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        x = self.res_block(x)\n",
    "        x = self.upsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.downsample = nn.ModuleList([\n",
    "            ResidualBlock(3, 64, 3, 0.1),\n",
    "            DownBlock(64, 128, 3, 0.1),\n",
    "            DownBlock(128, 256, 3, 0.1),\n",
    "            DownBlock(256, 512, 3, 0.1),\n",
    "            DownBlock(512, 1024, 3, 0.1),\n",
    "        ])\n",
    "        \n",
    "        self.upsample = nn.ModuleList([\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            UpBlock(1024 + 512, 512, 3, 0.1),\n",
    "            UpBlock(512 + 256, 256, 3, 0.1),\n",
    "            UpBlock(256 + 128, 128, 3, 0.1),\n",
    "        ])\n",
    "        \n",
    "        self.last_blocks = nn.ModuleList([\n",
    "            ResidualBlock(128 + 64, 64, 3, 0.1),\n",
    "            nn.Conv2d(64, 1, 3, padding='same'),\n",
    "            nn.Sigmoid(),\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for block in self.downsample:\n",
    "            x = block(x)\n",
    "            skips.append(x)\n",
    "        skips = reversed(skips[:-1])\n",
    "        for block, skip in zip(self.upsample, skips):\n",
    "            x = block(x)\n",
    "            x = torch.concat([x, skip], dim=1)\n",
    "        for block in self.last_blocks:\n",
    "            x = block(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(val_data_loader):\n",
    "    net.eval()\n",
    "    N = len(val_data_loader)\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    for images, labels in val_data_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = net(images)\n",
    "        loss += F.binary_cross_entropy(pred, labels)\n",
    "        acc += (pred.round() == labels).sum() / labels.numel()\n",
    "    loss /= N\n",
    "    acc /= N\n",
    "    return loss.item(), acc.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_batch(images, labels):\n",
    "    net.eval()\n",
    "    pred = net(images)\n",
    "    loss = F.binary_cross_entropy(pred, labels)\n",
    "    acc = (pred.round() == labels).sum() / labels.numel()\n",
    "    return loss.item(), acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(images, labels):\n",
    "    net.eval()\n",
    "    loss, acc = evaluate_batch(images, labels)\n",
    "    print(f'loss: {loss:.8f} - acc: {acc:.8f}')\n",
    "    mask_pred = net(images)\n",
    "    plot_images(images.cpu())\n",
    "    plot_images((images * 0.4 + 0.6 * mask_pred.unsqueeze(1)).cpu())\n",
    "    print('Predicted masks:')\n",
    "    plot_images(mask_pred.unsqueeze(1).cpu())\n",
    "    print('Ground truth:')\n",
    "    plot_images(labels.unsqueeze(1).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet()\n",
    "net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImagePairDataset(\n",
    "    img_dir='data/Kvasir_SEG_Training_880/image',\n",
    "    label_dir='data/Kvasir_SEG_Training_880/mask',\n",
    "    img_size=(128, 128)\n",
    ")\n",
    "val_ds = ImagePairDataset(\n",
    "    img_dir='data/Kvasir_SEG_Validation_120/images',\n",
    "    label_dir='data/Kvasir_SEG_Validation_120/masks',\n",
    "    img_size=(128, 128)\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(train_ds, batch_size=128, shuffle=True, prefetch_factor=2, num_workers=2)\n",
    "val_data_loader = DataLoader(val_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "best_weight_state = None\n",
    "\n",
    "for epoch in range(1, 1 + EPOCHS):\n",
    "    for images, labels in (bar := tqdm(train_data_loader)):\n",
    "        net.train()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = net(images)\n",
    "        loss = F.binary_cross_entropy(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            train_acc = (pred.detach().round() == labels).sum()\n",
    "            train_acc = train_acc.float() / labels.numel()\n",
    "            history['train_acc'].append(train_acc.item())\n",
    "            train_loss = loss.detach().item()\n",
    "            history['train_loss'].append(train_loss)\n",
    "        \n",
    "        val_loss, val_acc = evaluate(val_data_loader)\n",
    "        if all(val_acc > acc for acc in history['val_acc']):\n",
    "            best_weight_state = (net.state_dict(),\n",
    "                                 {'train_acc': train_acc.item(),\n",
    "                                  'val_acc': val_acc,\n",
    "                                  'epoch': epoch})\n",
    "        \n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        bar.set_description(\n",
    "            ' - '.join([\n",
    "                f'epoch: {epoch}/{EPOCHS}',\n",
    "                f'train_loss: {train_loss:.8f}',\n",
    "                f'train_acc: {train_acc:.8f}',\n",
    "                f'val_loss: {val_loss:.8f}',\n",
    "                f'val_acc: {val_acc:.8f}',\n",
    "            ])\n",
    "        )\n",
    "print(f'The best weight state: \\n{best_weight_state[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = iter(val_data_loader)\n",
    "net.load_state_dict(best_weight_state[0])\n",
    "\n",
    "images, labels = next(val_iter)\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "test(images[:16], labels[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_weight_state[0], 'best_weights.pt')\n",
    "torch.save(net.state_dict(), 'unet128.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
